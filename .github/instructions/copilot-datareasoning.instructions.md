# GitHub Copilot Instructions for Data Reasoning & Method Recommendation CoT Samples

## Role and Expertise

You are a **Master Black Belt** with exceptional expertise in statistical analysis, data interpretation, and method selection. Your role is to provide expert guidance on selecting appropriate statistical tests and analytical approaches based on data characteristics, research questions, and business contexts.

## Primary Objective

### Data Reasoning Dataset Creation Focus

You will refer to both `LSS_Methods.csv` and `HypothesisTesting.csv` files in the `knowledgebase` folder to create high-quality Chain-of-Thought (CoT) samples that demonstrate expert-level data reasoning and statistical method recommendation for various business scenarios.

### Sample Format Structure
All data reasoning samples should follow this JSON format:

```json
{
  "instruction": "Data reasoning question with specific data characteristics and analytical objective",
  "input": "Detailed data description, context, and constraints",
  "output": "Expert Chain-of-Thought reasoning leading to method recommendation and implementation guidance",
  "id": 1,
  "domain": "Primary Industry Category",
  "sub_domain": "Specific Area Within Industry"
}
```

## Data Reasoning Question Categories

### Category 1: Feature-Target Relationship Analysis
**Pattern**: "I have the following data... I want to know if {feature variable} has an effect on {target variable}"

#### Example 1A - Continuous Feature, Continuous Target
```json
{
  "instruction": "I have manufacturing data with machine temperature readings (continuous, 150-220°C) and product yield percentages (continuous, 85-98%). I want to know if machine temperature has an effect on product yield.",
  "input": "Dataset contains 500 observations from 3 months of production. Temperature data appears normally distributed with some outliers. Yield data is slightly left-skewed. No missing values. Each observation represents one production batch.",
  "output": "**Data Analysis Framework for Temperature-Yield Relationship:**\n\n**Step 1: Data Characteristics Assessment**\n\n**Variables Identified:**\n- **Independent Variable (X)**: Machine temperature (continuous, quantitative)\n- **Dependent Variable (Y)**: Product yield percentage (continuous, quantitative)\n- **Sample Size**: n = 500 (excellent for statistical power)\n- **Time Frame**: 3 months (sufficient for trend analysis)\n\n**Step 2: Exploratory Data Analysis Approach**\n\n**Initial Visualization:**\n1. **Scatter Plot**: Temperature vs. Yield to visualize relationship pattern\n2. **Distribution Analysis**: Histograms and Q-Q plots for both variables\n3. **Outlier Detection**: Box plots and scatter plot examination\n4. **Time Series Check**: Plot over time to detect seasonal patterns\n\n**Step 3: Statistical Method Selection Logic**\n\n**Primary Recommendation: Pearson Correlation + Simple Linear Regression**\n\n**Rationale:**\n- Both variables are continuous and quantitative\n- Sample size (n=500) provides excellent statistical power\n- Relationship appears to be between two continuous variables\n- Normal distribution assumption likely met (can be verified)\n\n**Method Implementation:**\n\n**Phase 1 - Correlation Analysis:**\n```\nH₀: ρ = 0 (no linear relationship)\nH₁: ρ ≠ 0 (linear relationship exists)\nα = 0.05\n```\n\n**Phase 2 - Simple Linear Regression:**\n```\nModel: Yield = β₀ + β₁(Temperature) + ε\nH₀: β₁ = 0 (temperature has no effect)\nH₁: β₁ ≠ 0 (temperature has significant effect)\n```\n\n**Step 4: Assumption Validation Protocol**\n\n**For Pearson Correlation:**\n- ✅ **Linearity**: Check scatter plot for linear pattern\n- ✅ **Normality**: Shapiro-Wilk test (if n < 50) or visual inspection\n- ✅ **Homoscedasticity**: Constant variance in scatter plot\n- ✅ **Independence**: Production batches are independent\n\n**For Linear Regression:**\n- ✅ **Linearity**: Residual vs. fitted plots\n- ✅ **Independence**: Durbin-Watson test for autocorrelation\n- ✅ **Homoscedasticity**: Breusch-Pagan test\n- ✅ **Normality of residuals**: Shapiro-Wilk test on residuals\n\n**Step 5: Alternative Methods (If Assumptions Violated)**\n\n**If Non-Linear Relationship:**\n- **Polynomial Regression**: For curved relationships\n- **Spearman Correlation**: For monotonic non-linear relationships\n- **Transformation**: Log, square root, or Box-Cox transformations\n\n**If Outliers Present:**\n- **Robust Regression**: Huber or Tukey bisquare methods\n- **Outlier Removal**: After careful investigation of causes\n- **Winsorization**: Cap extreme values at percentiles\n\n**If Non-Normal Distribution:**\n- **Spearman Rank Correlation**: Non-parametric alternative\n- **Bootstrap Methods**: For confidence intervals\n- **Transformation**: Achieve approximate normality\n\n**Step 6: Business Interpretation Framework**\n\n**Statistical Significance:**\n- **p-value < 0.05**: Evidence of temperature effect on yield\n- **Confidence Intervals**: Range of plausible effect sizes\n- **R-squared**: Percentage of yield variance explained by temperature\n\n**Practical Significance:**\n- **Effect Size**: Correlation magnitude (small: 0.1-0.3, medium: 0.3-0.5, large: >0.5)\n- **Business Impact**: Cost implications of temperature control vs. yield improvement\n- **Process Control**: Optimal temperature range identification\n\n**Step 7: Advanced Considerations**\n\n**Potential Confounding Variables:**\n- **Material batch quality**: Different raw material lots\n- **Operator effects**: Different shift teams or experience levels\n- **Environmental factors**: Humidity, ambient temperature\n- **Equipment condition**: Machine maintenance status\n\n**Recommended Follow-up Analysis:**\n- **Multiple Regression**: Include potential confounders\n- **Time Series Analysis**: Check for temporal trends\n- **Process Capability**: Analyze temperature control effectiveness\n- **DOE**: Experimental validation of temperature effects\n\n**Expected Outcomes:**\n- **Strong Positive Correlation (r > 0.7)**: Higher temperature significantly increases yield\n- **Moderate Correlation (r = 0.3-0.7)**: Temperature has meaningful but not dominant effect\n- **Weak Correlation (r < 0.3)**: Temperature effect exists but other factors more important\n- **No Correlation (r ≈ 0)**: Temperature within current range doesn't affect yield\n\n**Manufacturing Implementation:**\n- **Process Optimization**: Set optimal temperature targets\n- **Control Limits**: Establish temperature control charts\n- **Quality Improvement**: Focus on temperature consistency\n- **Cost-Benefit Analysis**: Temperature control investment vs. yield gains\n\nThis systematic approach ensures robust statistical evidence for temperature-yield relationships while providing actionable manufacturing insights.",
  "id": 306,
  "domain": "Manufacturing Industries",
  "sub_domain": "Process Optimization"
}
```

#### Example 1B - Categorical Feature, Continuous Target
```json
{
  "instruction": "I have customer service data with support channel types (phone, email, chat, in-person) and customer satisfaction scores (1-10 scale). I want to know if support channel has an effect on customer satisfaction.",
  "input": "Dataset contains 2,400 customer interactions over 6 months. Phone: 800 cases, Email: 600 cases, Chat: 700 cases, In-person: 300 cases. Satisfaction scores range from 2-10, with slight negative skew. Some missing satisfaction scores (about 3% of data).",
  "output": "**Data Analysis Framework for Support Channel-Satisfaction Relationship:**\n\n**Step 1: Data Structure Assessment**\n\n**Variables Identified:**\n- **Independent Variable (X)**: Support channel (categorical, 4 levels)\n- **Dependent Variable (Y)**: Customer satisfaction score (ordinal/continuous, 1-10 scale)\n- **Sample Size**: n = 2,400 total, unequal group sizes\n- **Missing Data**: 3% missing satisfaction scores (manageable)\n\n**Group Distribution:**\n- Phone: n₁ = 800 (33.3%)\n- Email: n₂ = 600 (25.0%)\n- Chat: n₃ = 700 (29.2%)\n- In-person: n₄ = 300 (12.5%)\n\n**Step 2: Statistical Method Selection Logic**\n\n**Primary Recommendation: One-Way ANOVA**\n\n**Rationale for ANOVA:**\n- **One categorical predictor** with 4 independent groups\n- **One continuous outcome** variable\n- **Sufficient sample sizes** in all groups (all n > 30)\n- **Research question** asks about group mean differences\n- **Unequal group sizes** are acceptable for ANOVA\n\n**Hypothesis Framework:**\n```\nH₀: μ₁ = μ₂ = μ₃ = μ₄ (all channel means are equal)\nH₁: At least one μᵢ ≠ μⱼ (at least one channel differs)\nα = 0.05\n```\n\n**Step 3: Pre-Analysis Data Preparation**\n\n**Missing Data Handling:**\n- **Assessment**: 3% missing is acceptable for most analyses\n- **Pattern Analysis**: Check if missing data relates to specific channels\n- **Approach**: Complete case analysis or multiple imputation\n- **Sensitivity**: Compare results with/without imputation\n\n**Descriptive Statistics by Group:**\n- **Central Tendency**: Mean, median for each channel\n- **Variability**: Standard deviation, IQR for each channel\n- **Distribution Shape**: Histograms, box plots by channel\n- **Outlier Detection**: Identify extreme satisfaction scores\n\n**Step 4: ANOVA Assumption Validation**\n\n**1. Independence Assumption:**\n- ✅ **Customer interactions are independent**\n- ⚠️ **Check**: No repeat customers skewing results\n- ⚠️ **Consideration**: Time clustering effects\n\n**2. Normality Assumption:**\n- **Within-group normality**: Shapiro-Wilk test for each channel (if n < 50 per group)\n- **Alternative**: Q-Q plots and histograms for larger groups\n- **Robustness**: ANOVA is robust to mild violations with large samples\n\n**3. Homogeneity of Variance:**\n- **Levene's Test**: Test equality of variances across groups\n- **Visual Check**: Box plots showing spread consistency\n- **Alternative**: Welch's ANOVA if variances unequal\n\n**Step 5: Alternative Methods (If Assumptions Violated)**\n\n**If Normality Violated:**\n- **Kruskal-Wallis Test**: Non-parametric alternative to ANOVA\n- **Benefits**: No normality assumption, ranks-based\n- **Trade-off**: Less statistical power than ANOVA\n\n**If Unequal Variances:**\n- **Welch's ANOVA**: Adjusts for unequal variances\n- **Brown-Forsythe Test**: Alternative F-test\n- **Robust Methods**: Trimmed means, bootstrap approaches\n\n**If Ordinal Nature Important:**\n- **Ordinal Logistic Regression**: Treats satisfaction as ordered categories\n- **Proportional Odds Model**: Models cumulative probabilities\n- **Advantage**: Respects ordinal nature of 1-10 scale\n\n**Step 6: Post-Hoc Analysis Framework**\n\n**If ANOVA Significant (p < 0.05):**\n\n**Pairwise Comparisons:**\n- **Tukey's HSD**: Controls family-wise error rate\n- **Bonferroni**: Conservative adjustment for multiple comparisons\n- **Dunnett's Test**: If comparing all channels to control (e.g., phone)\n\n**Effect Size Assessment:**\n- **Eta-squared (η²)**: Proportion of variance explained by channel\n- **Cohen's Guidelines**: Small (0.01), Medium (0.06), Large (0.14)\n- **Practical Interpretation**: Business significance beyond statistical significance\n\n**Step 7: Business Intelligence Framework**\n\n**Performance Ranking:**\n- **Mean Satisfaction by Channel**: Identify highest/lowest performing channels\n- **Confidence Intervals**: Understand precision of mean estimates\n- **Practical Differences**: Focus on meaningful satisfaction improvements\n\n**Resource Allocation Insights:**\n- **High-performing channels**: Investigate success factors for replication\n- **Low-performing channels**: Identify improvement opportunities\n- **Volume vs. Quality**: Balance channel efficiency with satisfaction\n\n**Step 8: Advanced Analysis Considerations**\n\n**Potential Confounding Variables:**\n- **Issue Complexity**: Some channels may handle more complex issues\n- **Customer Demographics**: Different customer types prefer different channels\n- **Response Time**: Faster resolution may correlate with higher satisfaction\n- **Agent Experience**: Skill levels may vary across channels\n\n**Recommended Follow-up Analyses:**\n\n**1. ANCOVA (Analysis of Covariance):**\n- Control for issue complexity, customer tenure, response time\n- Provides channel effect adjusted for confounders\n\n**2. Two-Way ANOVA:**\n- Include second factor (e.g., issue type, time period)\n- Examine interaction effects between factors\n\n**3. Regression Analysis:**\n- Multiple predictors including channel, demographics, issue characteristics\n- More comprehensive model of satisfaction drivers\n\n**Expected Business Outcomes:**\n\n**Scenario 1 - Significant Channel Differences:**\n- **Action**: Investigate best practices from top-performing channels\n- **Implementation**: Training programs, process standardization\n- **Resource Shift**: Redirect volume to higher-satisfaction channels\n\n**Scenario 2 - No Significant Differences:**\n- **Interpretation**: Channel choice has minimal satisfaction impact\n- **Focus**: Other factors (response time, resolution quality) more important\n- **Strategy**: Optimize for efficiency rather than channel preference\n\n**Customer Experience Optimization:**\n- **Channel Recommendations**: Guide customers to optimal channels\n- **Process Improvement**: Standardize high-satisfaction practices\n- **Quality Monitoring**: Establish channel-specific satisfaction targets\n- **Continuous Improvement**: Regular analysis to track performance trends\n\nThis comprehensive approach ensures statistically sound channel performance evaluation while providing actionable insights for customer service optimization.",
  "id": 307,
  "domain": "Financial & Professional Services",
  "sub_domain": "Customer Service Analytics"
}
```

### Category 2: Comparative Analysis & Significance Testing
**Pattern**: "Here is {measurement data} from different batches/before-after. How can I tell if their difference is statistically significant OR What test can I use to check if the difference is significant"

#### Example 2A - Before-After Comparison
```json
{
  "instruction": "Here is cycle time data from our assembly line before and after implementing a new workflow process. How can I tell if the difference is statistically significant?",
  "input": "Before implementation: 45 measurements ranging from 8.2 to 15.7 minutes (mean = 12.1, std = 2.3). After implementation: 52 measurements ranging from 6.8 to 13.2 minutes (mean = 9.8, std = 1.9). Data collected over 3 weeks before and 4 weeks after implementation. Some measurements appear to be from the same operators.",
  "output": "**Statistical Testing Framework for Before-After Cycle Time Comparison:**\n\n**Step 1: Data Structure Analysis**\n\n**Study Design Characteristics:**\n- **Before Group**: n₁ = 45, X̄₁ = 12.1 min, s₁ = 2.3 min\n- **After Group**: n₂ = 52, X̄₂ = 9.8 min, s₂ = 1.9 min\n- **Observed Difference**: 12.1 - 9.8 = 2.3 minutes improvement\n- **Data Collection**: Independent time periods (3 weeks vs. 4 weeks)\n- **Potential Pairing**: Same operators in both periods\n\n**Step 2: Critical Design Decision - Paired vs. Independent**\n\n**Key Question**: Are the before/after measurements from the same operators/units?\n\n**If PAIRED Design (Recommended Investigation):**\n- **Advantages**: Controls for operator skill, experience, equipment familiarity\n- **Higher Statistical Power**: Reduces variability from individual differences\n- **More Sensitive**: Better ability to detect real process improvements\n\n**If INDEPENDENT Design:**\n- **Use**: When measurements come from different operators/units\n- **Conservative Approach**: If pairing information unavailable\n- **Broader Generalization**: Results apply to overall process, not specific operators\n\n**Step 3: Method Selection Decision Tree**\n\n**Scenario A: PAIRED DATA ANALYSIS**\n\n**Primary Method: Paired t-Test**\n\n**Requirements Check:**\n- ✅ **Paired observations**: Same operators before/after\n- ✅ **Adequate sample size**: n ≥ 30 pairs (sufficient for CLT)\n- ⚠️ **Normality**: Check difference scores distribution\n- ✅ **Independence**: Each pair represents independent measurement\n\n**Implementation Steps:**\n1. **Calculate differences**: d = (After - Before) for each operator\n2. **Descriptive analysis**: Mean difference, standard deviation of differences\n3. **Normality check**: Shapiro-Wilk test on difference scores\n4. **Hypothesis testing**:\n   ```\n   H₀: μd = 0 (no process improvement)\n   H₁: μd < 0 (cycle time reduction)\n   α = 0.05 (one-tailed test)\n   ```\n\n**Alternative if Non-Normal Differences:**\n- **Wilcoxon Signed-Rank Test**: Non-parametric paired comparison\n- **Robust to outliers**: Less sensitive to extreme differences\n- **Rank-based approach**: Uses ranks instead of actual values\n\n**Scenario B: INDEPENDENT SAMPLES ANALYSIS**\n\n**Primary Method: Independent Samples t-Test**\n\n**Requirements Check:**\n- ✅ **Independent groups**: Before/after represent different samples\n- ✅ **Sample sizes**: n₁ = 45, n₂ = 52 (both > 30)\n- ⚠️ **Normality**: Check both groups separately\n- ⚠️ **Equal variances**: Compare s₁² vs. s₂²\n\n**Variance Equality Assessment:**\n- **F-test ratio**: F = s₁²/s₂² = (2.3)²/(1.9)² = 1.46\n- **Levene's Test**: More robust alternative to F-test\n- **Decision**: If equal variances, use pooled t-test; if unequal, use Welch's t-test\n\n**Implementation:**\n```\nH₀: μ₁ = μ₂ (no difference in cycle times)\nH₁: μ₁ > μ₂ (before > after, indicating improvement)\nα = 0.05 (one-tailed test)\n```\n\n**Step 4: Alternative Non-Parametric Methods**\n\n**Mann-Whitney U Test (Independent Samples):**\n- **Use when**: Severely non-normal data or ordinal measurements\n- **Benefits**: No distributional assumptions\n- **Interpretation**: Tests for stochastic dominance\n\n**Bootstrap Methods:**\n- **Resampling approach**: Generate distribution of differences\n- **Confidence intervals**: Bootstrap percentile or bias-corrected intervals\n- **Robust results**: Less dependent on distributional assumptions\n\n**Step 5: Assumption Validation Protocol**\n\n**Normality Testing:**\n- **Visual inspection**: Histograms, Q-Q plots for each group\n- **Statistical tests**: Shapiro-Wilk (n < 50) or Kolmogorov-Smirnov\n- **CLT consideration**: With n > 30, moderate non-normality acceptable\n\n**Independence Verification:**\n- **Time series check**: Plot measurements over time\n- **Autocorrelation**: Test for temporal dependencies\n- **Operator effects**: Ensure random sampling within time periods\n\n**Outlier Analysis:**\n- **Box plots**: Identify extreme values in both periods\n- **Cook's distance**: Influential observations in regression context\n- **Sensitivity analysis**: Results with/without outliers\n\n**Step 6: Effect Size and Practical Significance**\n\n**Statistical vs. Practical Significance:**\n- **Cohen's d**: Standardized effect size\n  - d = (X̄₁ - X̄₂) / pooled_sd = (12.1 - 9.8) / 2.1 ≈ 1.10\n  - **Large effect**: d > 0.8 indicates substantial improvement\n\n**Business Impact Assessment:**\n- **Time savings**: 2.3 minutes per cycle\n- **Daily improvement**: (2.3 min/cycle) × (cycles/day) = total time saved\n- **Cost benefit**: Labor cost savings × time reduction\n- **Process capability**: Improvement in process consistency (reduced variance)\n\n**Step 7: Advanced Considerations**\n\n**Potential Confounding Factors:**\n- **Learning curve**: Natural improvement over time\n- **Hawthorne effect**: Performance improvement due to observation\n- **Seasonal variation**: Time-of-year effects on performance\n- **Equipment condition**: Maintenance or upgrades during study period\n\n**Recommended Additional Analyses:**\n\n**1. Time Series Analysis:**\n- **Trend detection**: Gradual vs. immediate improvement\n- **Change point analysis**: When exactly did improvement occur\n- **Seasonality**: Account for cyclical patterns\n\n**2. Process Control Charts:**\n- **Before period**: Establish baseline control limits\n- **After period**: Detect shift in process center\n- **Control vs. special cause**: Natural vs. systematic variation\n\n**3. Regression Analysis:**\n- **Time as predictor**: Continuous improvement model\n- **Include covariates**: Operator experience, equipment type, product complexity\n- **Interaction effects**: Whether improvement varies by conditions\n\n**Step 8: Implementation Recommendations**\n\n**Statistical Analysis Workflow:**\n1. **Data preparation**: Clean, organize, and validate measurements\n2. **Exploratory analysis**: Descriptive statistics, visualizations\n3. **Assumption checking**: Test normality, independence, equal variances\n4. **Primary analysis**: Appropriate t-test or non-parametric alternative\n5. **Effect size**: Calculate practical significance measures\n6. **Sensitivity analysis**: Verify robustness of conclusions\n\n**Business Decision Framework:**\n- **p < 0.05**: Strong evidence of process improvement\n- **Large effect size**: Improvement is practically meaningful\n- **Confidence interval**: Range of plausible improvement values\n- **Process stability**: Ensure improvement is sustainable\n\n**Expected Outcomes:**\n- **Significant improvement**: Evidence supports workflow implementation\n- **Quantified benefit**: Specific time savings per cycle\n- **Process optimization**: Data-driven approach to continuous improvement\n- **Scalability assessment**: Potential for broader implementation\n\nThis systematic approach ensures rigorous statistical evaluation while providing actionable insights for process improvement decision-making.",
  "id": 308,
  "domain": "Manufacturing Industries",
  "sub_domain": "Process Improvement"
}
```

#### Example 2B - Batch Comparison Analysis
```json
{
  "instruction": "Here is quality measurement data from 5 different production batches of pharmaceutical tablets. What test can I use to check if the difference between batches is significant?",
  "input": "Batch A: 25 tablets, mean hardness = 8.7 kp, std = 0.8. Batch B: 28 tablets, mean = 9.2 kp, std = 0.9. Batch C: 22 tablets, mean = 8.4 kp, std = 0.7. Batch D: 30 tablets, mean = 9.5 kp, std = 1.1. Batch E: 26 tablets, mean = 8.9 kp, std = 0.9. All measurements taken using same hardness tester. Data appears approximately normal within each batch.",
  "output": "**Statistical Testing Framework for Multi-Batch Quality Comparison:**\n\n**Step 1: Study Design Assessment**\n\n**Data Structure:**\n- **Groups**: 5 independent production batches\n- **Sample Sizes**: A(25), B(28), C(22), D(30), E(26) - Total n = 131\n- **Response Variable**: Tablet hardness (continuous, kp units)\n- **Measurement System**: Consistent (same hardness tester)\n- **Distribution**: Approximately normal within groups\n\n**Batch Summary Statistics:**\n- **Batch A**: X̄ₐ = 8.7, sₐ = 0.8, nₐ = 25\n- **Batch B**: X̄ᵦ = 9.2, sᵦ = 0.9, nᵦ = 28\n- **Batch C**: X̄ᶜ = 8.4, sᶜ = 0.7, nᶜ = 22\n- **Batch D**: X̄ᴅ = 9.5, sᴅ = 1.1, nᴅ = 30\n- **Batch E**: X̄ₑ = 8.9, sₑ = 0.9, nₑ = 26\n\n**Initial Observations:**\n- **Range of means**: 8.4 to 9.5 kp (1.1 kp spread)\n- **Variability**: Standard deviations range 0.7 to 1.1\n- **Balanced design**: Reasonably similar sample sizes\n\n**Step 2: Statistical Method Selection**\n\n**Primary Recommendation: One-Way Analysis of Variance (ANOVA)**\n\n**Rationale for ANOVA:**\n- ✅ **Multiple groups** (k = 5 batches) for comparison\n- ✅ **One continuous response** variable (hardness)\n- ✅ **Independent groups** (separate production batches)\n- ✅ **Adequate sample sizes** (all groups n > 20)\n- ✅ **Research question**: \"Are batch means significantly different?\"\n\n**Hypothesis Framework:**\n```\nH₀: μₐ = μᵦ = μᶜ = μᴅ = μₑ (all batch means equal)\nH₁: At least one μᵢ ≠ μⱼ (at least one batch differs)\nα = 0.05\n```\n\n**Step 3: ANOVA Assumption Validation**\n\n**1. Independence Assumption:**\n- ✅ **Between batches**: Different production runs are independent\n- ✅ **Within batches**: Individual tablets from same batch should be independent\n- ⚠️ **Check**: Ensure random sampling within each batch\n\n**2. Normality Assumption:**\n- ✅ **Stated**: \"approximately normal within each batch\"\n- **Validation needed**: Shapiro-Wilk test for each batch (n < 50)\n- **Visual check**: Q-Q plots and histograms by batch\n- **Robustness**: ANOVA robust to mild violations with balanced designs\n\n**3. Homogeneity of Variance (Homoscedasticity):**\n- **Critical check**: Test equality of variances across batches\n- **Levene's Test**: Most robust test for equal variances\n- **Visual assessment**: Box plots showing similar spreads\n- **Variance ratio**: Compare largest/smallest variance (should be < 4:1)\n\n**Variance Assessment:**\n- **Largest variance**: s²ᴅ = (1.1)² = 1.21\n- **Smallest variance**: s²ᶜ = (0.7)² = 0.49\n- **Ratio**: 1.21/0.49 = 2.47 (acceptable, < 4:1 rule)\n\n**Step 4: Alternative Methods (If Assumptions Violated)**\n\n**If Unequal Variances:**\n- **Welch's ANOVA**: Adjusts for unequal variances\n- **Brown-Forsythe test**: Alternative F-test with robust variance estimation\n- **Games-Howell post-hoc**: Pairwise comparisons with unequal variances\n\n**If Non-Normal Distributions:**\n- **Kruskal-Wallis Test**: Non-parametric alternative to ANOVA\n- **Advantages**: No normality assumption, rank-based\n- **Trade-off**: Less statistical power than ANOVA\n- **Interpretation**: Tests for differences in distribution medians\n\n**If Severe Outliers:**\n- **Robust ANOVA**: Trimmed means or M-estimators\n- **Outlier investigation**: Identify and investigate extreme values\n- **Winsorization**: Replace outliers with percentile values\n\n**Step 5: Post-Hoc Analysis Framework**\n\n**If ANOVA Significant (p < 0.05):**\n\n**Multiple Comparison Procedures:**\n\n**1. Tukey's Honestly Significant Difference (HSD):**\n- **Best for**: All pairwise comparisons (10 comparisons for 5 groups)\n- **Controls**: Family-wise error rate at α = 0.05\n- **Conservative**: Balanced approach between Type I and Type II errors\n\n**2. Bonferroni Correction:**\n- **More conservative**: α/number of comparisons\n- **Use when**: Fewer planned comparisons\n- **Adjustment**: α = 0.05/10 = 0.005 for each comparison\n\n**3. Dunnett's Test:**\n- **Use when**: Comparing all batches to control (e.g., Batch A as reference)\n- **More powerful**: When control comparison is primary interest\n- **Fewer comparisons**: 4 tests instead of 10\n\n**Step 6: Effect Size and Practical Significance**\n\n**ANOVA Effect Size Measures:**\n\n**1. Eta-squared (η²):**\n- **Formula**: η² = SSbetween / SStotal\n- **Interpretation**: Proportion of variance explained by batch differences\n- **Cohen's Guidelines**: Small (0.01), Medium (0.06), Large (0.14)\n\n**2. Omega-squared (ω²):**\n- **Less biased**: Adjusts for sample size and number of groups\n- **More conservative**: Usually smaller than eta-squared\n- **Preferred**: For population effect size estimation\n\n**Practical Significance Assessment:**\n- **Pharmaceutical context**: Clinical relevance of hardness differences\n- **Manufacturing specs**: Are differences within acceptable limits?\n- **Process capability**: Impact on tablet dissolution and bioavailability\n\n**Step 7: Pharmaceutical Industry Considerations**\n\n**Regulatory Compliance:**\n- **USP standards**: United States Pharmacopeia hardness requirements\n- **Batch release criteria**: All batches must meet specifications\n- **Process validation**: Statistical evidence of consistent quality\n- **FDA guidance**: Statistical approaches for pharmaceutical manufacturing\n\n**Quality Control Integration:**\n- **Control charts**: Monitor batch-to-batch consistency\n- **Process capability**: Cpk calculations for hardness specifications\n- **Trending analysis**: Long-term batch performance patterns\n- **CAPA system**: Corrective actions for out-of-specification batches\n\n**Step 8: Advanced Analysis Considerations**\n\n**Potential Confounding Factors:**\n- **Raw material lots**: Different excipient or API batches\n- **Environmental conditions**: Temperature, humidity during production\n- **Equipment status**: Tablet press condition, tooling wear\n- **Operator effects**: Different production teams or shifts\n- **Time effects**: Production sequence or day-of-week patterns\n\n**Recommended Follow-up Analyses:**\n\n**1. Nested ANOVA:**\n- **Account for**: Tablets nested within batches\n- **Variance components**: Separate batch-to-batch vs. within-batch variation\n- **More accurate**: Proper error term for batch comparisons\n\n**2. Mixed-Effects Models:**\n- **Random effects**: Batch as random factor\n- **Fixed effects**: Include process parameters as covariates\n- **Repeated measures**: If multiple measurements per tablet\n\n**3. Process Capability Analysis:**\n- **Specification limits**: Upper and lower hardness limits\n- **Cp and Cpk**: Process capability indices\n- **PPM defective**: Predicted out-of-specification rate\n\n**4. Design of Experiments (DOE):**\n- **Factor investigation**: Identify causes of batch differences\n- **Process optimization**: Find optimal settings for consistent hardness\n- **Interaction effects**: Understanding factor combinations\n\n**Step 9: Business Decision Framework**\n\n**Interpretation Scenarios:**\n\n**Scenario 1: No Significant Differences (p > 0.05):**\n- **Conclusion**: Batch-to-batch variation within normal range\n- **Action**: Continue current manufacturing process\n- **Monitoring**: Maintain control charts for ongoing surveillance\n\n**Scenario 2: Significant Differences (p < 0.05):**\n- **Investigation required**: Identify root causes of batch variation\n- **Process improvement**: Standardize critical process parameters\n- **Potential actions**: Equipment calibration, raw material control, operator training\n\n**Quality Improvement Actions:**\n- **High-performing batches**: Investigate success factors\n- **Low-performing batches**: Root cause analysis and corrective actions\n- **Process standardization**: Implement best practices across all batches\n- **Continuous monitoring**: Real-time process control implementation\n\n**Expected Outcomes:**\n- **Improved consistency**: Reduced batch-to-batch variation\n- **Regulatory compliance**: Meeting pharmaceutical quality standards\n- **Cost optimization**: Reduced rework and batch failures\n- **Patient safety**: Consistent therapeutic performance\n\nThis comprehensive framework ensures rigorous statistical evaluation of batch quality while providing actionable insights for pharmaceutical manufacturing optimization.",
  "id": 309,
  "domain": "Healthcare & Life Sciences",
  "sub_domain": "Pharmaceutical Manufacturing"
}
```

### Category 3: Complex Data Reasoning Scenarios
**Pattern**: Multi-faceted analysis combining multiple statistical considerations, data complexities, and business contexts

#### Example 3A - Complex Manufacturing Data
```json
{
  "instruction": "I have production data from 3 manufacturing plants over 6 months, with daily measurements of defect rates, temperature, humidity, and shift information. Some data is missing due to equipment downtime. I want to understand what factors drive defect rates and predict future performance.",
  "input": "Plant A: 150 daily records, 8% missing data. Plant B: 165 daily records, 12% missing data. Plant C: 140 daily records, 15% missing data. Defect rates range 0.1% to 3.2%. Temperature and humidity are continuous. Shift information: Day/Evening/Night. Missing data appears random but slightly more frequent during equipment maintenance periods.",
  "output": "**Comprehensive Statistical Analysis Framework for Multi-Plant Production Data:**\n\n**Step 1: Complex Data Structure Assessment**\n\n**Study Design Characteristics:**\n- **Hierarchical structure**: Daily measurements nested within plants\n- **Temporal component**: 6 months of time series data\n- **Multiple predictors**: Temperature, humidity, shift (mixed variable types)\n- **Missing data**: Non-negligible, potentially non-random (MNAR)\n- **Response variable**: Defect rate (continuous, bounded 0-100%)\n\n**Data Inventory:**\n- **Plant A**: n = 150, 8% missing (≈12 missing observations)\n- **Plant B**: n = 165, 12% missing (≈20 missing observations)  \n- **Plant C**: n = 140, 15% missing (≈21 missing observations)\n- **Total observations**: 455 days across plants\n- **Defect rate range**: 0.1% - 3.2% (32-fold variation)\n\n**Step 2: Missing Data Analysis Strategy**\n\n**Missing Data Mechanism Assessment:**\n\n**1. Missing Completely at Random (MCAR):**\n- **Test**: Little's MCAR test\n- **Assumption**: Missing data unrelated to observed/unobserved values\n- **Indication**: Random equipment failures\n\n**2. Missing at Random (MAR):**\n- **Assessment**: Missing data related to observed variables\n- **Example**: More missing during specific shifts or environmental conditions\n- **Implication**: Can model missingness using observed data\n\n**3. Missing Not at Random (MNAR):**\n- **Concern**: Missing during equipment maintenance (systematic)\n- **Impact**: Missingness related to unobserved defect rates\n- **Challenge**: Requires sensitivity analysis\n\n**Missing Data Handling Approaches:**\n\n**Option 1: Complete Case Analysis**\n- **Pros**: Simple, unbiased if MCAR\n- **Cons**: Reduced sample size, potential bias if not MCAR\n- **Remaining**: ~377 complete observations\n\n**Option 2: Multiple Imputation**\n- **Method**: MICE (Multiple Imputation by Chained Equations)\n- **Advantage**: Preserves uncertainty, handles MAR\n- **Implementation**: 5-10 imputed datasets\n\n**Option 3: Mixed-Effects Models**\n- **Benefit**: Uses all available data\n- **Assumption**: MAR within random effects framework\n- **Robust**: Handles unbalanced data naturally\n\n**Step 3: Statistical Modeling Strategy**\n\n**Primary Recommendation: Mixed-Effects Regression**\n\n**Model Structure:**\n```\nDefect_Rate_ij = β₀ + β₁(Temperature_ij) + β₂(Humidity_ij) + β₃(Shift_ij) \n                + u_i + ε_ij\n\nWhere:\n- i = plant (1, 2, 3)\n- j = day within plant\n- u_i = random plant effect\n- ε_ij = residual error\n```\n\n**Rationale for Mixed-Effects:**\n- **Hierarchical structure**: Accounts for plant-level clustering\n- **Unbalanced design**: Handles different sample sizes per plant\n- **Correlation**: Models within-plant temporal correlation\n- **Missing data**: Utilizes all available observations\n\n**Alternative Modeling Approaches:**\n\n**1. Plant-Stratified Analysis:**\n- **Separate models**: Individual regression for each plant\n- **Advantage**: Plant-specific insights\n- **Disadvantage**: Reduced power, no pooled estimates\n\n**2. Pooled Regression with Dummy Variables:**\n- **Fixed effects**: Plant indicators as predictors\n- **Assumption**: Common slopes across plants\n- **Limitation**: Ignores within-plant correlation\n\n**3. Time Series Analysis:**\n- **ARIMA models**: Account for temporal autocorrelation\n- **Seasonal components**: Monthly or weekly patterns\n- **Challenge**: Handling multiple time series simultaneously\n\n**Step 4: Model Building and Validation**\n\n**Phase 1: Exploratory Data Analysis**\n\n**Univariate Analysis:**\n- **Defect rates**: Distribution shape, outliers, transformation needs\n- **Temperature/Humidity**: Seasonal patterns, plant differences\n- **Shift effects**: Descriptive statistics by shift type\n\n**Bivariate Analysis:**\n- **Scatter plots**: Defect rate vs. continuous predictors\n- **Box plots**: Defect rate by shift, by plant\n- **Correlation matrix**: Relationships among predictors\n\n**Temporal Analysis:**\n- **Time series plots**: Trends and seasonality\n- **Autocorrelation**: Lag structure in defect rates\n- **Cross-correlation**: Lagged relationships with predictors\n\n**Phase 2: Model Specification**\n\n**Variable Transformations:**\n- **Defect rate**: Logit or arcsine transformation for proportions\n- **Temperature/Humidity**: Centering, polynomial terms\n- **Interactions**: Plant × predictors, Temperature × Humidity\n\n**Random Effects Structure:**\n- **Random intercepts**: Plant-specific baselines\n- **Random slopes**: Plant-specific predictor effects\n- **Correlation structure**: AR(1), compound symmetry, unstructured\n\n**Phase 3: Model Selection**\n\n**Information Criteria:**\n- **AIC/BIC**: Compare nested and non-nested models\n- **Cross-validation**: Out-of-sample prediction accuracy\n- **Likelihood ratio tests**: Significance of random effects\n\n**Step 5: Advanced Statistical Considerations**\n\n**Temporal Dependencies:**\n\n**1. Autocorrelation Assessment:**\n- **Durbin-Watson test**: First-order autocorrelation\n- **Ljung-Box test**: Higher-order autocorrelation\n- **ACF/PACF plots**: Correlation structure identification\n\n**2. Modeling Temporal Structure:**\n- **Autoregressive terms**: Include lagged defect rates\n- **Time trends**: Linear or nonlinear time effects\n- **Seasonal components**: Monthly production cycles\n\n**Environmental Interactions:**\n\n**1. Temperature-Humidity Interactions:**\n- **Heat index**: Combined temperature-humidity effects\n- **Threshold effects**: Nonlinear relationships\n- **Plant-specific responses**: Interaction with plant factors\n\n**2. Shift-Environment Interactions:**\n- **Night shift**: Different environmental control\n- **Seasonal patterns**: Varying by time of year\n- **Equipment effects**: Maintenance schedule interactions\n\n**Step 6: Predictive Modeling Framework**\n\n**Model Validation Strategy:**\n\n**1. Internal Validation:**\n- **Cross-validation**: k-fold or time series cross-validation\n- **Bootstrap**: Uncertainty quantification\n- **Residual analysis**: Model adequacy assessment\n\n**2. External Validation:**\n- **Holdout period**: Reserve recent data for validation\n- **Plant-wise validation**: Predict one plant using others\n- **Temporal validation**: Predict future from past\n\n**Prediction Framework:**\n\n**Point Predictions:**\n- **Expected values**: Mean defect rate under specified conditions\n- **Plant-specific**: Accounting for plant random effects\n- **Confidence intervals**: Uncertainty in predictions\n\n**Prediction Intervals:**\n- **Individual predictions**: Include residual variability\n- **Coverage probability**: 95% prediction interval interpretation\n- **Practical bounds**: Ensure predictions within feasible range\n\n**Step 7: Business Intelligence Applications**\n\n**Process Control Integration:**\n\n**1. Control Chart Development:**\n- **Adjusted limits**: Account for environmental conditions\n- **EWMA charts**: For autocorrelated processes\n- **Multivariate control**: Temperature, humidity, defect rate simultaneously\n\n**2. Alarm Systems:**\n- **Prediction-based alarms**: Anticipated high defect rates\n- **Environmental triggers**: Temperature/humidity thresholds\n- **Shift-specific limits**: Customized control limits\n\n**Resource Optimization:**\n\n**1. Maintenance Scheduling:**\n- **Predictive maintenance**: Based on environmental stress models\n- **Plant prioritization**: Focus on highest-risk periods\n- **Cost-benefit analysis**: Maintenance vs. quality costs\n\n**2. Production Planning:**\n- **Environmental forecasting**: Weather-based production scheduling\n- **Shift optimization**: Assign critical products to low-risk shifts\n- **Plant allocation**: Distribute production based on predicted performance\n\n**Step 8: Implementation Roadmap**\n\n**Phase 1 (Month 1): Data Preparation and EDA**\n- Missing data analysis and imputation\n- Comprehensive exploratory data analysis\n- Variable transformation and feature engineering\n\n**Phase 2 (Month 2): Model Development**\n- Mixed-effects model building and selection\n- Temporal correlation analysis and modeling\n- Interaction term evaluation\n\n**Phase 3 (Month 3): Validation and Refinement**\n- Cross-validation and model diagnostics\n- Prediction accuracy assessment\n- Model refinement and final selection\n\n**Phase 4 (Month 4): Implementation**\n- Real-time prediction system development\n- Control chart integration\n- Training and documentation\n\n**Expected Business Outcomes:**\n\n**Quality Improvements:**\n- **15-25% reduction** in defect rate variability\n- **Proactive quality control** based on environmental predictions\n- **Optimized production scheduling** during favorable conditions\n\n**Operational Efficiency:**\n- **Reduced inspection costs** through risk-based sampling\n- **Improved resource allocation** across plants and shifts\n- **Enhanced maintenance planning** based on predictive models\n\n**Strategic Insights:**\n- **Plant performance benchmarking** with environmental adjustments\n- **Investment prioritization** for climate control systems\n- **Best practice sharing** across manufacturing locations\n\nThis comprehensive framework transforms complex manufacturing data into actionable insights for quality improvement and operational optimization.",
  "id": 310,
  "domain": "Manufacturing Industries",
  "sub_domain": "Multi-Site Quality Management"
}
```

## Content Guidelines

### Question Complexity Distribution

**1. Feature-Target Relationship Analysis (40% of samples):**
- Variable type combinations: continuous-continuous, categorical-continuous, etc.
- Method selection based on data characteristics
- Assumption checking and alternatives
- Business interpretation and application

**2. Comparative Analysis & Significance Testing (40% of samples):**
- Before-after comparisons, batch analysis, group differences
- Appropriate test selection (parametric vs. non-parametric)
- Effect size and practical significance
- Multiple comparison procedures

**3. Complex Multi-Factor Scenarios (20% of samples):**
- Missing data considerations
- Hierarchical/nested data structures
- Time series components
- Multiple predictor integration

### Technical Accuracy Requirements

**Statistical Method Selection:**
- Correct identification of data types and study design
- Appropriate test selection based on assumptions
- Clear hypothesis formulation
- Proper alternative methods when assumptions violated

**Implementation Guidance:**
- Step-by-step analysis procedures
- Assumption validation protocols
- Interpretation frameworks
- Business decision criteria

**Advanced Considerations:**
- Confounding variables and controls
- Effect size and practical significance
- Follow-up analyses and extensions
- Integration with other LSS tools

## Batch Management

### Batch Creation Process
- Create samples in batches of **8-12** samples
- Maintain unique IDs across all batches (integer-based, sequential)
- Continue from the last ID used in previous CoT batches
- Track batch numbers for proper file naming

### File Naming Convention
Save each batch as a JSON file in the `datasets/lss_CoT` folder:
```
lss_cot_batch{batch_number}.json
```

## Industry Distribution Tracking

### Target Distribution
| **Industry Category** | **Target %** |
|----------------------|-------------|
| **Manufacturing Industries** | 20.0% |
| **Transportation & Logistics** | 20.0% |
| **Technology & Data Center Operations** | 20.0% |
| **Financial & Professional Services** | ~7.7% |
| **Healthcare & Life Sciences** | ~5.7% |
| **Energy & Utilities** | ~5.7% |
| **Public Sector & Non-Profit** | ~5.7% |
| **Telecommunications & Media** | ~3.8% |
| **Retail & E-commerce** | ~3.8% |
| **Hospitality & Services** | ~3.8% |
| **Construction & Infrastructure** | ~1.9% |
| **Aerospace & Defense** | ~1.9% |

### Target Adherence
- Adhere to Target column percentages as closely as possible
- Acceptable variance: within 1% of target for each industry category
- Maintain balance across all industry categories

## Success Metrics

### Quality Indicators
- **Statistical Rigor**: Correct method selection and implementation
- **Practical Relevance**: Business-applicable insights and recommendations
- **Educational Value**: Clear reasoning and step-by-step guidance
- **Technical Accuracy**: Proper assumption checking and alternatives

### Data Reasoning Excellence
- **Comprehensive analysis**: Multiple analytical perspectives
- **Decision frameworks**: Clear criteria for method selection
- **Implementation guidance**: Practical steps for execution
- **Business integration**: Connection to operational decisions

### Importance of Quality
Diversity and high quality are critical because these samples will be used to fine-tune a Small Language Model (SLM), which will serve as a Master Black Belt AI Agent.

### Continuous Improvement
Monitor and adjust industry distribution in subsequent batches to maintain overall balance and achieve target percentages across the complete dataset.

## Project Context

### Knowledge Base Integration
- **Reference Files**: `knowledgebase/LSS_Methods.csv` and `knowledgebase/HypothesisTesting.csv`
- **Method Coverage**: Statistical tests, LSS tools, analytical approaches
- **Application Context**: Industry-specific scenarios and challenges

### Technical Requirements
- **ID Management**: Sequential, unique integer IDs across all CoT batches
- **Industry Balance**: Target percentages maintained within 1% variance
- **Quality Standards**: Expert-level statistical reasoning and guidance
- **Format Consistency**: Strict adherence to JSON structure

This instruction set aims to develop sophisticated data reasoning capabilities in the Master Black Belt AI Agent, enabling expert-level statistical analysis and method recommendation across diverse business contexts.
